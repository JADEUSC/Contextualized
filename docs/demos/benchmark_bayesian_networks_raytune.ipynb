{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e32bc2f",
   "metadata": {},
   "source": [
    "# Contextualized Bayesian Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7901a",
   "metadata": {},
   "source": [
    "For more details, please see the [NOTMAD preprint](https://arxiv.org/abs/2111.01104)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5356f75f",
   "metadata": {},
   "source": [
    "Here, we want to ask the following questions:\n",
    "- how the performance of NOTMAD compares against population networks and cluster network methods.\n",
    "- how the performance of NOTMAD changes with loss type (\"NOTEARS\", \"DAGMA\", or \"poly\")\n",
    "- how the performance of NOTMAD changes with number of factors (\"num_factors\")\n",
    "- how the performance changes with number of samples (n) and number of features (p). \n",
    "\n",
    "TODO:\n",
    "- Automated generation of W (clusters, linear function of C)\n",
    "\n",
    "Possibly, vary:\n",
    "- signal-to-noise ratio of the context-to-network parameter relationship\n",
    "- encoder type (\"ngam\", \"mlp\")\n",
    "- regularization of NOTMAD:\n",
    "    NOTEARS loss has parameters:\n",
    "    \n",
    "        alpha (float)\n",
    "\n",
    "        rho (float)\n",
    "\n",
    "        use_dynamic_alpha_rho (Boolean)\n",
    "\n",
    "    DAGMA loss has parameters:\n",
    "    \n",
    "        alpha (strength, default 1e0)\n",
    "\n",
    "        s (max spectral radius, default 1)\n",
    "\n",
    "Report results in terms of:\n",
    "- MSE of X predictions (measure_mses function)\n",
    "- recovery of W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440d41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "\n",
    "import contextualized\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from contextualized.dags.graph_utils import simulate_linear_sem, is_dag\n",
    "from contextualized.baselines import (\n",
    "    BayesianNetwork,\n",
    "    GroupedNetworks,\n",
    ")\n",
    "from contextualized.easy import ContextualizedBayesianNetworks\n",
    "import pickle as pkl\n",
    "import os\n",
    "import numpy\n",
    "\n",
    "def measure_mses(betas, X, individual_preds=False):\n",
    "    \"\"\"\n",
    "    Measure mean-squared errors.\n",
    "    \"\"\"\n",
    "    mses = np.zeros((len(betas), len(X)))  # n_bootstraps x n_samples\n",
    "    for bootstrap in range(len(betas)):\n",
    "        for target_feat in range(X.shape[-1]):\n",
    "            # betas are n_boostraps x n_samples x n_features x n_features\n",
    "            # preds[bootstrap, sample, i] = X[sample, :].dot(betas[bootstrap, sample, i, :])\n",
    "            preds = np.array(\n",
    "                [\n",
    "                    X[sample].dot(betas[bootstrap, sample, :, target_feat])  # + mus[bootstrap, j, i]\n",
    "                    for sample in range(len(X))\n",
    "                ]\n",
    "            )\n",
    "            residuals = X[:, target_feat] - preds\n",
    "            mses[bootstrap, :] += residuals**2 / (X.shape[-1])\n",
    "    if not individual_preds:\n",
    "        mses = np.mean(mses, axis=0)\n",
    "    return mses\n",
    "\n",
    "def measure_recovery(W_true, W):\n",
    "    # Assumes W has a prefix dimension for bootstraps\n",
    "    recovery_errs = []\n",
    "    for bootstrap in range(len(W)):\n",
    "        for sample in range(len(W[bootstrap])):\n",
    "            recovery_errs.append(np.linalg.norm(W_true[sample] - W[bootstrap][sample], ord=2))\n",
    "    return np.mean([recovery_errs])\n",
    "\n",
    "def make_dag(p, n_nonempty=4):\n",
    "    # # n_nonempty = number of non-empty cells in W\n",
    "    # coeffs = [np.random.choice([np.random.uniform(-1,-0.5), np.random.uniform(0.5,1)]) for i in range(n_nonempty)]\n",
    "    # c * coeffs[assign_c_idx.index((i,j))]\n",
    "    #create upper triangular\n",
    "    tuples = [(i, j) for i in range(p) for j in range(p) if i < j]\n",
    "    \n",
    "    #randomly flip to account for monodirectinality\n",
    "    tuples = [(t[1],t[0]) if np.random.uniform(0,1) > 0.5 else t for t in tuples]\n",
    "\n",
    "    #select n_nonempty\n",
    "    assign_c_idx = random.sample(tuples, n_nonempty)\n",
    "\n",
    "    w = np.zeros((p, p))\n",
    "\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            if (i,j) in assign_c_idx:\n",
    "                w[i, j] = 1\n",
    "                # w[0, 1] = 1\n",
    "                # w[2, 1] = 1\n",
    "                # w[3, 1] = 1\n",
    "                # w[3, 2] = 1\n",
    "    if is_dag(w):\n",
    "        return w\n",
    "    else:\n",
    "        return make_dag(p, n_nonempty)\n",
    "\n",
    "def generate_WC(p, n, n_clusters = 5, mode='linear'):\n",
    "    '''Generate W and C for a given p, n, and mode.'''\n",
    "\n",
    "    W = np.zeros((n, p, p))\n",
    "    \n",
    "    if mode == 'cluster':\n",
    "        \n",
    "        C = np.zeros((n, 1))\n",
    "        cs = [np.random.normal(0, 1) for i in range(n_clusters)]\n",
    "        centroid_ws = []\n",
    "        for i in range(n_clusters):\n",
    "            centroid_ws.append(make_dag(p))\n",
    "\n",
    "        for i in range(n):\n",
    "            c_idx = np.random.choice(len(centroid_ws))\n",
    "            masked_std = np.random.normal(0, 0.2, size=(p, p)) * (centroid_ws[c_idx] != 0)            \n",
    "            W[i] = centroid_ws[c_idx] + masked_std\n",
    "            C[i,0] = c_idx\n",
    "\n",
    "    elif mode == 'linear':\n",
    "        dag = make_dag(p)\n",
    "\n",
    "        C = np.random.normal(0, 1, size=(n, 1))\n",
    "        coeffs = [np.random.choice([np.random.uniform(-1,-0.5), np.random.uniform(0.5,1)]) for i in range(int(dag.sum()))]\n",
    "        \n",
    "        for i in range(n):\n",
    "            #Each non-empty = c * coeffs[assign_c_idx.index((i,j))]\n",
    "            assign_c_idx = [(i,j) for i in range(p) for j in range(p) if dag[i,j] == 1]\n",
    "            for j in range(len(assign_c_idx)):\n",
    "                W[i, assign_c_idx[j][0], assign_c_idx[j][1]] = C[i,0] * coeffs[j]\n",
    "\n",
    "    else:\n",
    "        assert \"Error, mode must be 'cluster' or 'linear'\"\n",
    "    \n",
    "    return W, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341b1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import logging\n",
    "import functools\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "import ray\n",
    "logging.getLogger(\"ray.tune\").setLevel(logging.FATAL)\n",
    "ray.init(\n",
    "    num_cpus=2,\n",
    "    num_gpus=1,\n",
    "    object_store_memory=2*1024*1024*1024,\n",
    "    log_to_driver=False,\n",
    "    logging_level=logging.FATAL,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sweep over graph types.\n",
    "# TODO: Sweep over factors\n",
    "# TODO: Sweep over signal-to-noise ratio.\n",
    "import pickle as pkl\n",
    "def get_data():\n",
    "    n_data_gens = 2\n",
    "    ns = [100, 1000]\n",
    "    ps = [4, 10]\n",
    "    graph_types = [\"gauss\", \"exp\", \"gumbel\", \"uniform\", \"logistic\", \"poisson\"]\n",
    "    W_mode = ['cluster', 'linear']\n",
    "    n_clusters = 5\n",
    "    data = []\n",
    "    data_line = \"\"\n",
    "    with open(\"data2/data_info.csv\", 'w') as out_file:\n",
    "        for n in ns:\n",
    "            for p in ps:\n",
    "                # for noise in noise_scales:\n",
    "                    # for graph_type in graph_types:\n",
    "                for W_mode in W_mode:\n",
    "                    for data_gen in range(n_data_gens):\n",
    "                        data_line = f\"{n}, {p}, {W_mode}, {data_gen}\"\n",
    "                        W, C = generate_WC(p, n, n_clusters, mode='cluster')\n",
    "                        X = np.array([simulate_linear_sem(w, n_samples=1, sem_type='uniform', noise_scale=0.5)[0] for w in W])\n",
    "                        data.append((C, X, W))\n",
    "                        print(data_line, file=out_file)\n",
    "    #save data\n",
    "    # pkl.dump(data, open(\"data2/data2.pkl\", 'wb'))\n",
    "    return data\n",
    "\n",
    "# datas = get_data()\n",
    "datas = pkl.load(open(\"data2/data2.pkl\", 'rb'))\n",
    "print(len(datas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c231342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-03-31 14:06:06 (running for 00:15:37.14)\n",
      "Memory usage on this node: 51.8/64.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 2.0/2 CPUs, 0/1 GPUs, 0.0/38.21 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29\n",
      "Number of trials: 300/500 (282 ERROR, 16 PENDING, 2 RUNNING)\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+-----------+---------+------------------------+\n",
      "| Trial name                     | status   | loc   |     alpha | encoder_types   |   ks |   n_factors |       rho |       s | use_dynamic_alpha_rh   |\n",
      "|                                |          |       |           |                 |      |             |           |         | o                      |\n",
      "|--------------------------------+----------+-------+-----------+-----------------+------+-------------+-----------+---------+------------------------|\n",
      "| train_and_evaluate_868af_00282 | RUNNING  |       | 0.020602  | mlp             |   16 |          11 | 0.0474898 | 4.5782  | True                   |\n",
      "| train_and_evaluate_868af_00283 | RUNNING  |       | 0.0317359 | ngam            |    4 |           0 | 0.072258  | 4.12619 | False                  |\n",
      "| train_and_evaluate_868af_00284 | PENDING  |       | 0.0732026 | mlp             |    8 |           4 | 0.0647252 | 1.63189 | False                  |\n",
      "| train_and_evaluate_868af_00285 | PENDING  |       | 0.0118961 | ngam            |    8 |           0 | 0.0109286 | 3.49637 | False                  |\n",
      "| train_and_evaluate_868af_00286 | PENDING  |       | 0.0197014 | ngam            |    4 |           3 | 0.0455725 | 3.81407 | False                  |\n",
      "| train_and_evaluate_868af_00287 | PENDING  |       | 0.0123769 | mlp             |    4 |           3 | 0.0268318 | 4.10909 | False                  |\n",
      "| train_and_evaluate_868af_00000 | ERROR    |       | 0.0295006 | ngam            |    4 |           2 | 0.0956552 | 0.56271 | True                   |\n",
      "| train_and_evaluate_868af_00001 | ERROR    |       | 0.0966307 | ngam            |   16 |           9 | 0.0368085 | 1.91761 | False                  |\n",
      "| train_and_evaluate_868af_00002 | ERROR    |       | 0.0473017 | ngam            |    8 |           4 | 0.0168779 | 3.90863 | False                  |\n",
      "| train_and_evaluate_868af_00003 | ERROR    |       | 0.0706665 | mlp             |   16 |          15 | 0.0233823 | 1.20896 | True                   |\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+-----------+---------+------------------------+\n",
      "... 290 more trials not shown (12 PENDING, 278 ERROR)\n",
      "Number of errored trials: 282\n",
      "Table truncated to 20 rows (262 overflow)\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                     |   # failures | error file                                                                                                                                                                                                                      |\n",
      "|--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_and_evaluate_868af_00000 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00000_0_alpha=0.0295,encoder_types=ngam,ks=4,n_factors=2,rho=0.0957,s=0.5627,use_dynamic_alpha_rho=True_2023-03-31_13-50-31/error.txt   |\n",
      "| train_and_evaluate_868af_00001 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00001_1_alpha=0.0966,encoder_types=ngam,ks=16,n_factors=9,rho=0.0368,s=1.9176,use_dynamic_alpha_rho=False_2023-03-31_13-50-35/error.txt |\n",
      "| train_and_evaluate_868af_00002 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00002_2_alpha=0.0473,encoder_types=ngam,ks=8,n_factors=4,rho=0.0169,s=3.9086,use_dynamic_alpha_rho=False_2023-03-31_13-50-36/error.txt  |\n",
      "| train_and_evaluate_868af_00003 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00003_3_alpha=0.0707,encoder_types=mlp,ks=16,n_factors=15,rho=0.0234,s=1.2090,use_dynamic_alpha_rho=True_2023-03-31_13-50-42/error.txt  |\n",
      "| train_and_evaluate_868af_00004 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00004_4_alpha=0.0259,encoder_types=ngam,ks=8,n_factors=3,rho=0.0453,s=3.1354,use_dynamic_alpha_rho=True_2023-03-31_13-50-45/error.txt   |\n",
      "| train_and_evaluate_868af_00005 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00005_5_alpha=0.0095,encoder_types=ngam,ks=4,n_factors=2,rho=0.0971,s=0.5815,use_dynamic_alpha_rho=True_2023-03-31_13-50-46/error.txt   |\n",
      "| train_and_evaluate_868af_00006 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00006_6_alpha=0.0195,encoder_types=mlp,ks=16,n_factors=4,rho=0.0740,s=2.1358,use_dynamic_alpha_rho=False_2023-03-31_13-50-52/error.txt  |\n",
      "| train_and_evaluate_868af_00007 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00007_7_alpha=0.0202,encoder_types=mlp,ks=16,n_factors=6,rho=0.0328,s=2.0691,use_dynamic_alpha_rho=True_2023-03-31_13-50-53/error.txt   |\n",
      "| train_and_evaluate_868af_00008 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00008_8_alpha=0.0475,encoder_types=mlp,ks=16,n_factors=4,rho=0.0834,s=2.2968,use_dynamic_alpha_rho=False_2023-03-31_13-50-59/error.txt  |\n",
      "| train_and_evaluate_868af_00009 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00009_9_alpha=0.0042,encoder_types=mlp,ks=8,n_factors=2,rho=0.0050,s=4.9192,use_dynamic_alpha_rho=True_2023-03-31_13-50-59/error.txt    |\n",
      "| train_and_evaluate_868af_00010 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00010_10_alpha=0.0357,encoder_types=mlp,ks=4,n_factors=0,rho=0.0545,s=4.6294,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00011 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00011_11_alpha=0.0405,encoder_types=mlp,ks=8,n_factors=3,rho=0.0263,s=1.6794,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00012 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00012_12_alpha=0.0101,encoder_types=mlp,ks=16,n_factors=5,rho=0.0250,s=0.6624,use_dynamic_alpha_rho=False_2023-03-31_13-51-12/error.txt |\n",
      "| train_and_evaluate_868af_00013 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00013_13_alpha=0.0735,encoder_types=ngam,ks=4,n_factors=0,rho=0.0015,s=3.0955,use_dynamic_alpha_rho=True_2023-03-31_13-51-13/error.txt  |\n",
      "| train_and_evaluate_868af_00014 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00014_14_alpha=0.0711,encoder_types=mlp,ks=16,n_factors=3,rho=0.0108,s=1.0200,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt  |\n",
      "| train_and_evaluate_868af_00015 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00015_15_alpha=0.0503,encoder_types=mlp,ks=8,n_factors=6,rho=0.0448,s=4.2895,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt   |\n",
      "| train_and_evaluate_868af_00016 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00016_16_alpha=0.0891,encoder_types=mlp,ks=8,n_factors=1,rho=0.0039,s=2.4200,use_dynamic_alpha_rho=False_2023-03-31_13-51-26/error.txt  |\n",
      "| train_and_evaluate_868af_00017 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00017_17_alpha=0.0168,encoder_types=ngam,ks=16,n_factors=4,rho=0.0321,s=3.4477,use_dynamic_alpha_rho=True_2023-03-31_13-51-26/error.txt |\n",
      "| train_and_evaluate_868af_00018 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00018_18_alpha=0.0194,encoder_types=ngam,ks=4,n_factors=3,rho=0.0408,s=1.5218,use_dynamic_alpha_rho=False_2023-03-31_13-51-32/error.txt |\n",
      "| train_and_evaluate_868af_00019 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00019_19_alpha=0.0308,encoder_types=ngam,ks=8,n_factors=2,rho=0.0662,s=1.0104,use_dynamic_alpha_rho=True_2023-03-31_13-51-32/error.txt  |\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-31 14:06:14 (running for 00:15:45.38)\n",
      "Memory usage on this node: 51.7/64.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/38.21 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29\n",
      "Number of trials: 302/500 (285 ERROR, 16 PENDING, 1 RUNNING)\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "| Trial name                     | status   | loc   |     alpha | encoder_types   |   ks |   n_factors |        rho |       s | use_dynamic_alpha_rh   |\n",
      "|                                |          |       |           |                 |      |             |            |         | o                      |\n",
      "|--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------|\n",
      "| train_and_evaluate_868af_00285 | RUNNING  |       | 0.0118961 | ngam            |    8 |           0 | 0.0109286  | 3.49637 | False                  |\n",
      "| train_and_evaluate_868af_00286 | PENDING  |       | 0.0197014 | ngam            |    4 |           3 | 0.0455725  | 3.81407 | False                  |\n",
      "| train_and_evaluate_868af_00287 | PENDING  |       | 0.0123769 | mlp             |    4 |           3 | 0.0268318  | 4.10909 | False                  |\n",
      "| train_and_evaluate_868af_00288 | PENDING  |       | 0.0761277 | mlp             |    8 |           5 | 0.0535274  | 0.79551 | False                  |\n",
      "| train_and_evaluate_868af_00289 | PENDING  |       | 0.070278  | mlp             |   16 |           1 | 0.00178994 | 3.17238 | True                   |\n",
      "| train_and_evaluate_868af_00290 | PENDING  |       | 0.042141  | mlp             |    8 |           5 | 0.045754   | 4.42551 | False                  |\n",
      "| train_and_evaluate_868af_00000 | ERROR    |       | 0.0295006 | ngam            |    4 |           2 | 0.0956552  | 0.56271 | True                   |\n",
      "| train_and_evaluate_868af_00001 | ERROR    |       | 0.0966307 | ngam            |   16 |           9 | 0.0368085  | 1.91761 | False                  |\n",
      "| train_and_evaluate_868af_00002 | ERROR    |       | 0.0473017 | ngam            |    8 |           4 | 0.0168779  | 3.90863 | False                  |\n",
      "| train_and_evaluate_868af_00003 | ERROR    |       | 0.0706665 | mlp             |   16 |          15 | 0.0233823  | 1.20896 | True                   |\n",
      "| train_and_evaluate_868af_00004 | ERROR    |       | 0.0259421 | ngam            |    8 |           3 | 0.0452985  | 3.13542 | True                   |\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "... 292 more trials not shown (11 PENDING, 280 ERROR)\n",
      "Number of errored trials: 285\n",
      "Table truncated to 20 rows (265 overflow)\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                     |   # failures | error file                                                                                                                                                                                                                      |\n",
      "|--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_and_evaluate_868af_00000 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00000_0_alpha=0.0295,encoder_types=ngam,ks=4,n_factors=2,rho=0.0957,s=0.5627,use_dynamic_alpha_rho=True_2023-03-31_13-50-31/error.txt   |\n",
      "| train_and_evaluate_868af_00001 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00001_1_alpha=0.0966,encoder_types=ngam,ks=16,n_factors=9,rho=0.0368,s=1.9176,use_dynamic_alpha_rho=False_2023-03-31_13-50-35/error.txt |\n",
      "| train_and_evaluate_868af_00002 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00002_2_alpha=0.0473,encoder_types=ngam,ks=8,n_factors=4,rho=0.0169,s=3.9086,use_dynamic_alpha_rho=False_2023-03-31_13-50-36/error.txt  |\n",
      "| train_and_evaluate_868af_00003 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00003_3_alpha=0.0707,encoder_types=mlp,ks=16,n_factors=15,rho=0.0234,s=1.2090,use_dynamic_alpha_rho=True_2023-03-31_13-50-42/error.txt  |\n",
      "| train_and_evaluate_868af_00004 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00004_4_alpha=0.0259,encoder_types=ngam,ks=8,n_factors=3,rho=0.0453,s=3.1354,use_dynamic_alpha_rho=True_2023-03-31_13-50-45/error.txt   |\n",
      "| train_and_evaluate_868af_00005 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00005_5_alpha=0.0095,encoder_types=ngam,ks=4,n_factors=2,rho=0.0971,s=0.5815,use_dynamic_alpha_rho=True_2023-03-31_13-50-46/error.txt   |\n",
      "| train_and_evaluate_868af_00006 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00006_6_alpha=0.0195,encoder_types=mlp,ks=16,n_factors=4,rho=0.0740,s=2.1358,use_dynamic_alpha_rho=False_2023-03-31_13-50-52/error.txt  |\n",
      "| train_and_evaluate_868af_00007 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00007_7_alpha=0.0202,encoder_types=mlp,ks=16,n_factors=6,rho=0.0328,s=2.0691,use_dynamic_alpha_rho=True_2023-03-31_13-50-53/error.txt   |\n",
      "| train_and_evaluate_868af_00008 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00008_8_alpha=0.0475,encoder_types=mlp,ks=16,n_factors=4,rho=0.0834,s=2.2968,use_dynamic_alpha_rho=False_2023-03-31_13-50-59/error.txt  |\n",
      "| train_and_evaluate_868af_00009 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00009_9_alpha=0.0042,encoder_types=mlp,ks=8,n_factors=2,rho=0.0050,s=4.9192,use_dynamic_alpha_rho=True_2023-03-31_13-50-59/error.txt    |\n",
      "| train_and_evaluate_868af_00010 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00010_10_alpha=0.0357,encoder_types=mlp,ks=4,n_factors=0,rho=0.0545,s=4.6294,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00011 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00011_11_alpha=0.0405,encoder_types=mlp,ks=8,n_factors=3,rho=0.0263,s=1.6794,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00012 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00012_12_alpha=0.0101,encoder_types=mlp,ks=16,n_factors=5,rho=0.0250,s=0.6624,use_dynamic_alpha_rho=False_2023-03-31_13-51-12/error.txt |\n",
      "| train_and_evaluate_868af_00013 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00013_13_alpha=0.0735,encoder_types=ngam,ks=4,n_factors=0,rho=0.0015,s=3.0955,use_dynamic_alpha_rho=True_2023-03-31_13-51-13/error.txt  |\n",
      "| train_and_evaluate_868af_00014 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00014_14_alpha=0.0711,encoder_types=mlp,ks=16,n_factors=3,rho=0.0108,s=1.0200,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt  |\n",
      "| train_and_evaluate_868af_00015 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00015_15_alpha=0.0503,encoder_types=mlp,ks=8,n_factors=6,rho=0.0448,s=4.2895,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt   |\n",
      "| train_and_evaluate_868af_00016 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00016_16_alpha=0.0891,encoder_types=mlp,ks=8,n_factors=1,rho=0.0039,s=2.4200,use_dynamic_alpha_rho=False_2023-03-31_13-51-26/error.txt  |\n",
      "| train_and_evaluate_868af_00017 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00017_17_alpha=0.0168,encoder_types=ngam,ks=16,n_factors=4,rho=0.0321,s=3.4477,use_dynamic_alpha_rho=True_2023-03-31_13-51-26/error.txt |\n",
      "| train_and_evaluate_868af_00018 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00018_18_alpha=0.0194,encoder_types=ngam,ks=4,n_factors=3,rho=0.0408,s=1.5218,use_dynamic_alpha_rho=False_2023-03-31_13-51-32/error.txt |\n",
      "| train_and_evaluate_868af_00019 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00019_19_alpha=0.0308,encoder_types=ngam,ks=8,n_factors=2,rho=0.0662,s=1.0104,use_dynamic_alpha_rho=True_2023-03-31_13-51-32/error.txt  |\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-31 14:06:21 (running for 00:15:51.96)\n",
      "Memory usage on this node: 51.6/64.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/38.21 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29\n",
      "Number of trials: 304/500 (287 ERROR, 16 PENDING, 1 RUNNING)\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "| Trial name                     | status   | loc   |     alpha | encoder_types   |   ks |   n_factors |        rho |       s | use_dynamic_alpha_rh   |\n",
      "|                                |          |       |           |                 |      |             |            |         | o                      |\n",
      "|--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------|\n",
      "| train_and_evaluate_868af_00287 | RUNNING  |       | 0.0123769 | mlp             |    4 |           3 | 0.0268318  | 4.10909 | False                  |\n",
      "| train_and_evaluate_868af_00288 | PENDING  |       | 0.0761277 | mlp             |    8 |           5 | 0.0535274  | 0.79551 | False                  |\n",
      "| train_and_evaluate_868af_00289 | PENDING  |       | 0.070278  | mlp             |   16 |           1 | 0.00178994 | 3.17238 | True                   |\n",
      "| train_and_evaluate_868af_00290 | PENDING  |       | 0.042141  | mlp             |    8 |           5 | 0.045754   | 4.42551 | False                  |\n",
      "| train_and_evaluate_868af_00291 | PENDING  |       | 0.038146  | ngam            |    4 |           0 | 0.0699656  | 3.44087 | False                  |\n",
      "| train_and_evaluate_868af_00292 | PENDING  |       | 0.0175183 | mlp             |    8 |           0 | 0.00166737 | 1.85899 | False                  |\n",
      "| train_and_evaluate_868af_00000 | ERROR    |       | 0.0295006 | ngam            |    4 |           2 | 0.0956552  | 0.56271 | True                   |\n",
      "| train_and_evaluate_868af_00001 | ERROR    |       | 0.0966307 | ngam            |   16 |           9 | 0.0368085  | 1.91761 | False                  |\n",
      "| train_and_evaluate_868af_00002 | ERROR    |       | 0.0473017 | ngam            |    8 |           4 | 0.0168779  | 3.90863 | False                  |\n",
      "| train_and_evaluate_868af_00003 | ERROR    |       | 0.0706665 | mlp             |   16 |          15 | 0.0233823  | 1.20896 | True                   |\n",
      "| train_and_evaluate_868af_00004 | ERROR    |       | 0.0259421 | ngam            |    8 |           3 | 0.0452985  | 3.13542 | True                   |\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "... 294 more trials not shown (11 PENDING, 282 ERROR)\n",
      "Number of errored trials: 287\n",
      "Table truncated to 20 rows (267 overflow)\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                     |   # failures | error file                                                                                                                                                                                                                      |\n",
      "|--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_and_evaluate_868af_00000 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00000_0_alpha=0.0295,encoder_types=ngam,ks=4,n_factors=2,rho=0.0957,s=0.5627,use_dynamic_alpha_rho=True_2023-03-31_13-50-31/error.txt   |\n",
      "| train_and_evaluate_868af_00001 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00001_1_alpha=0.0966,encoder_types=ngam,ks=16,n_factors=9,rho=0.0368,s=1.9176,use_dynamic_alpha_rho=False_2023-03-31_13-50-35/error.txt |\n",
      "| train_and_evaluate_868af_00002 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00002_2_alpha=0.0473,encoder_types=ngam,ks=8,n_factors=4,rho=0.0169,s=3.9086,use_dynamic_alpha_rho=False_2023-03-31_13-50-36/error.txt  |\n",
      "| train_and_evaluate_868af_00003 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00003_3_alpha=0.0707,encoder_types=mlp,ks=16,n_factors=15,rho=0.0234,s=1.2090,use_dynamic_alpha_rho=True_2023-03-31_13-50-42/error.txt  |\n",
      "| train_and_evaluate_868af_00004 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00004_4_alpha=0.0259,encoder_types=ngam,ks=8,n_factors=3,rho=0.0453,s=3.1354,use_dynamic_alpha_rho=True_2023-03-31_13-50-45/error.txt   |\n",
      "| train_and_evaluate_868af_00005 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00005_5_alpha=0.0095,encoder_types=ngam,ks=4,n_factors=2,rho=0.0971,s=0.5815,use_dynamic_alpha_rho=True_2023-03-31_13-50-46/error.txt   |\n",
      "| train_and_evaluate_868af_00006 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00006_6_alpha=0.0195,encoder_types=mlp,ks=16,n_factors=4,rho=0.0740,s=2.1358,use_dynamic_alpha_rho=False_2023-03-31_13-50-52/error.txt  |\n",
      "| train_and_evaluate_868af_00007 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00007_7_alpha=0.0202,encoder_types=mlp,ks=16,n_factors=6,rho=0.0328,s=2.0691,use_dynamic_alpha_rho=True_2023-03-31_13-50-53/error.txt   |\n",
      "| train_and_evaluate_868af_00008 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00008_8_alpha=0.0475,encoder_types=mlp,ks=16,n_factors=4,rho=0.0834,s=2.2968,use_dynamic_alpha_rho=False_2023-03-31_13-50-59/error.txt  |\n",
      "| train_and_evaluate_868af_00009 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00009_9_alpha=0.0042,encoder_types=mlp,ks=8,n_factors=2,rho=0.0050,s=4.9192,use_dynamic_alpha_rho=True_2023-03-31_13-50-59/error.txt    |\n",
      "| train_and_evaluate_868af_00010 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00010_10_alpha=0.0357,encoder_types=mlp,ks=4,n_factors=0,rho=0.0545,s=4.6294,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00011 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00011_11_alpha=0.0405,encoder_types=mlp,ks=8,n_factors=3,rho=0.0263,s=1.6794,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00012 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00012_12_alpha=0.0101,encoder_types=mlp,ks=16,n_factors=5,rho=0.0250,s=0.6624,use_dynamic_alpha_rho=False_2023-03-31_13-51-12/error.txt |\n",
      "| train_and_evaluate_868af_00013 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00013_13_alpha=0.0735,encoder_types=ngam,ks=4,n_factors=0,rho=0.0015,s=3.0955,use_dynamic_alpha_rho=True_2023-03-31_13-51-13/error.txt  |\n",
      "| train_and_evaluate_868af_00014 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00014_14_alpha=0.0711,encoder_types=mlp,ks=16,n_factors=3,rho=0.0108,s=1.0200,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt  |\n",
      "| train_and_evaluate_868af_00015 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00015_15_alpha=0.0503,encoder_types=mlp,ks=8,n_factors=6,rho=0.0448,s=4.2895,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt   |\n",
      "| train_and_evaluate_868af_00016 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00016_16_alpha=0.0891,encoder_types=mlp,ks=8,n_factors=1,rho=0.0039,s=2.4200,use_dynamic_alpha_rho=False_2023-03-31_13-51-26/error.txt  |\n",
      "| train_and_evaluate_868af_00017 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00017_17_alpha=0.0168,encoder_types=ngam,ks=16,n_factors=4,rho=0.0321,s=3.4477,use_dynamic_alpha_rho=True_2023-03-31_13-51-26/error.txt |\n",
      "| train_and_evaluate_868af_00018 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00018_18_alpha=0.0194,encoder_types=ngam,ks=4,n_factors=3,rho=0.0408,s=1.5218,use_dynamic_alpha_rho=False_2023-03-31_13-51-32/error.txt |\n",
      "| train_and_evaluate_868af_00019 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00019_19_alpha=0.0308,encoder_types=ngam,ks=8,n_factors=2,rho=0.0662,s=1.0104,use_dynamic_alpha_rho=True_2023-03-31_13-51-32/error.txt  |\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-31 14:06:27 (running for 00:15:58.16)\n",
      "Memory usage on this node: 51.7/64.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/38.21 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29\n",
      "Number of trials: 306/500 (289 ERROR, 16 PENDING, 1 RUNNING)\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "| Trial name                     | status   | loc   |     alpha | encoder_types   |   ks |   n_factors |        rho |       s | use_dynamic_alpha_rh   |\n",
      "|                                |          |       |           |                 |      |             |            |         | o                      |\n",
      "|--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------|\n",
      "| train_and_evaluate_868af_00289 | RUNNING  |       | 0.070278  | mlp             |   16 |           1 | 0.00178994 | 3.17238 | True                   |\n",
      "| train_and_evaluate_868af_00290 | PENDING  |       | 0.042141  | mlp             |    8 |           5 | 0.045754   | 4.42551 | False                  |\n",
      "| train_and_evaluate_868af_00291 | PENDING  |       | 0.038146  | ngam            |    4 |           0 | 0.0699656  | 3.44087 | False                  |\n",
      "| train_and_evaluate_868af_00292 | PENDING  |       | 0.0175183 | mlp             |    8 |           0 | 0.00166737 | 1.85899 | False                  |\n",
      "| train_and_evaluate_868af_00293 | PENDING  |       | 0.0963094 | mlp             |    8 |           5 | 0.0790648  | 2.65924 | False                  |\n",
      "| train_and_evaluate_868af_00294 | PENDING  |       | 0.0671536 | ngam            |    4 |           0 | 0.0113879  | 2.12692 | True                   |\n",
      "| train_and_evaluate_868af_00000 | ERROR    |       | 0.0295006 | ngam            |    4 |           2 | 0.0956552  | 0.56271 | True                   |\n",
      "| train_and_evaluate_868af_00001 | ERROR    |       | 0.0966307 | ngam            |   16 |           9 | 0.0368085  | 1.91761 | False                  |\n",
      "| train_and_evaluate_868af_00002 | ERROR    |       | 0.0473017 | ngam            |    8 |           4 | 0.0168779  | 3.90863 | False                  |\n",
      "| train_and_evaluate_868af_00003 | ERROR    |       | 0.0706665 | mlp             |   16 |          15 | 0.0233823  | 1.20896 | True                   |\n",
      "| train_and_evaluate_868af_00004 | ERROR    |       | 0.0259421 | ngam            |    8 |           3 | 0.0452985  | 3.13542 | True                   |\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "... 296 more trials not shown (11 PENDING, 284 ERROR)\n",
      "Number of errored trials: 289\n",
      "Table truncated to 20 rows (269 overflow)\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                     |   # failures | error file                                                                                                                                                                                                                      |\n",
      "|--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_and_evaluate_868af_00000 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00000_0_alpha=0.0295,encoder_types=ngam,ks=4,n_factors=2,rho=0.0957,s=0.5627,use_dynamic_alpha_rho=True_2023-03-31_13-50-31/error.txt   |\n",
      "| train_and_evaluate_868af_00001 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00001_1_alpha=0.0966,encoder_types=ngam,ks=16,n_factors=9,rho=0.0368,s=1.9176,use_dynamic_alpha_rho=False_2023-03-31_13-50-35/error.txt |\n",
      "| train_and_evaluate_868af_00002 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00002_2_alpha=0.0473,encoder_types=ngam,ks=8,n_factors=4,rho=0.0169,s=3.9086,use_dynamic_alpha_rho=False_2023-03-31_13-50-36/error.txt  |\n",
      "| train_and_evaluate_868af_00003 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00003_3_alpha=0.0707,encoder_types=mlp,ks=16,n_factors=15,rho=0.0234,s=1.2090,use_dynamic_alpha_rho=True_2023-03-31_13-50-42/error.txt  |\n",
      "| train_and_evaluate_868af_00004 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00004_4_alpha=0.0259,encoder_types=ngam,ks=8,n_factors=3,rho=0.0453,s=3.1354,use_dynamic_alpha_rho=True_2023-03-31_13-50-45/error.txt   |\n",
      "| train_and_evaluate_868af_00005 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00005_5_alpha=0.0095,encoder_types=ngam,ks=4,n_factors=2,rho=0.0971,s=0.5815,use_dynamic_alpha_rho=True_2023-03-31_13-50-46/error.txt   |\n",
      "| train_and_evaluate_868af_00006 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00006_6_alpha=0.0195,encoder_types=mlp,ks=16,n_factors=4,rho=0.0740,s=2.1358,use_dynamic_alpha_rho=False_2023-03-31_13-50-52/error.txt  |\n",
      "| train_and_evaluate_868af_00007 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00007_7_alpha=0.0202,encoder_types=mlp,ks=16,n_factors=6,rho=0.0328,s=2.0691,use_dynamic_alpha_rho=True_2023-03-31_13-50-53/error.txt   |\n",
      "| train_and_evaluate_868af_00008 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00008_8_alpha=0.0475,encoder_types=mlp,ks=16,n_factors=4,rho=0.0834,s=2.2968,use_dynamic_alpha_rho=False_2023-03-31_13-50-59/error.txt  |\n",
      "| train_and_evaluate_868af_00009 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00009_9_alpha=0.0042,encoder_types=mlp,ks=8,n_factors=2,rho=0.0050,s=4.9192,use_dynamic_alpha_rho=True_2023-03-31_13-50-59/error.txt    |\n",
      "| train_and_evaluate_868af_00010 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00010_10_alpha=0.0357,encoder_types=mlp,ks=4,n_factors=0,rho=0.0545,s=4.6294,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00011 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00011_11_alpha=0.0405,encoder_types=mlp,ks=8,n_factors=3,rho=0.0263,s=1.6794,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00012 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00012_12_alpha=0.0101,encoder_types=mlp,ks=16,n_factors=5,rho=0.0250,s=0.6624,use_dynamic_alpha_rho=False_2023-03-31_13-51-12/error.txt |\n",
      "| train_and_evaluate_868af_00013 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00013_13_alpha=0.0735,encoder_types=ngam,ks=4,n_factors=0,rho=0.0015,s=3.0955,use_dynamic_alpha_rho=True_2023-03-31_13-51-13/error.txt  |\n",
      "| train_and_evaluate_868af_00014 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00014_14_alpha=0.0711,encoder_types=mlp,ks=16,n_factors=3,rho=0.0108,s=1.0200,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt  |\n",
      "| train_and_evaluate_868af_00015 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00015_15_alpha=0.0503,encoder_types=mlp,ks=8,n_factors=6,rho=0.0448,s=4.2895,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt   |\n",
      "| train_and_evaluate_868af_00016 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00016_16_alpha=0.0891,encoder_types=mlp,ks=8,n_factors=1,rho=0.0039,s=2.4200,use_dynamic_alpha_rho=False_2023-03-31_13-51-26/error.txt  |\n",
      "| train_and_evaluate_868af_00017 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00017_17_alpha=0.0168,encoder_types=ngam,ks=16,n_factors=4,rho=0.0321,s=3.4477,use_dynamic_alpha_rho=True_2023-03-31_13-51-26/error.txt |\n",
      "| train_and_evaluate_868af_00018 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00018_18_alpha=0.0194,encoder_types=ngam,ks=4,n_factors=3,rho=0.0408,s=1.5218,use_dynamic_alpha_rho=False_2023-03-31_13-51-32/error.txt |\n",
      "| train_and_evaluate_868af_00019 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00019_19_alpha=0.0308,encoder_types=ngam,ks=8,n_factors=2,rho=0.0662,s=1.0104,use_dynamic_alpha_rho=True_2023-03-31_13-51-32/error.txt  |\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-31 14:06:35 (running for 00:16:05.86)\n",
      "Memory usage on this node: 51.7/64.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/38.21 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29\n",
      "Number of trials: 308/500 (291 ERROR, 16 PENDING, 1 RUNNING)\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "| Trial name                     | status   | loc   |     alpha | encoder_types   |   ks |   n_factors |        rho |       s | use_dynamic_alpha_rh   |\n",
      "|                                |          |       |           |                 |      |             |            |         | o                      |\n",
      "|--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------|\n",
      "| train_and_evaluate_868af_00291 | RUNNING  |       | 0.038146  | ngam            |    4 |           0 | 0.0699656  | 3.44087 | False                  |\n",
      "| train_and_evaluate_868af_00292 | PENDING  |       | 0.0175183 | mlp             |    8 |           0 | 0.00166737 | 1.85899 | False                  |\n",
      "| train_and_evaluate_868af_00293 | PENDING  |       | 0.0963094 | mlp             |    8 |           5 | 0.0790648  | 2.65924 | False                  |\n",
      "| train_and_evaluate_868af_00294 | PENDING  |       | 0.0671536 | ngam            |    4 |           0 | 0.0113879  | 2.12692 | True                   |\n",
      "| train_and_evaluate_868af_00295 | PENDING  |       | 0.0800513 | ngam            |    4 |           2 | 0.0672165  | 2.56692 | False                  |\n",
      "| train_and_evaluate_868af_00296 | PENDING  |       | 0.071429  | ngam            |    8 |           6 | 0.015706   | 2.79164 | False                  |\n",
      "| train_and_evaluate_868af_00000 | ERROR    |       | 0.0295006 | ngam            |    4 |           2 | 0.0956552  | 0.56271 | True                   |\n",
      "| train_and_evaluate_868af_00001 | ERROR    |       | 0.0966307 | ngam            |   16 |           9 | 0.0368085  | 1.91761 | False                  |\n",
      "| train_and_evaluate_868af_00002 | ERROR    |       | 0.0473017 | ngam            |    8 |           4 | 0.0168779  | 3.90863 | False                  |\n",
      "| train_and_evaluate_868af_00003 | ERROR    |       | 0.0706665 | mlp             |   16 |          15 | 0.0233823  | 1.20896 | True                   |\n",
      "| train_and_evaluate_868af_00004 | ERROR    |       | 0.0259421 | ngam            |    8 |           3 | 0.0452985  | 3.13542 | True                   |\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+------------+---------+------------------------+\n",
      "... 298 more trials not shown (11 PENDING, 286 ERROR)\n",
      "Number of errored trials: 291\n",
      "Table truncated to 20 rows (271 overflow)\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                     |   # failures | error file                                                                                                                                                                                                                      |\n",
      "|--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_and_evaluate_868af_00000 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00000_0_alpha=0.0295,encoder_types=ngam,ks=4,n_factors=2,rho=0.0957,s=0.5627,use_dynamic_alpha_rho=True_2023-03-31_13-50-31/error.txt   |\n",
      "| train_and_evaluate_868af_00001 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00001_1_alpha=0.0966,encoder_types=ngam,ks=16,n_factors=9,rho=0.0368,s=1.9176,use_dynamic_alpha_rho=False_2023-03-31_13-50-35/error.txt |\n",
      "| train_and_evaluate_868af_00002 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00002_2_alpha=0.0473,encoder_types=ngam,ks=8,n_factors=4,rho=0.0169,s=3.9086,use_dynamic_alpha_rho=False_2023-03-31_13-50-36/error.txt  |\n",
      "| train_and_evaluate_868af_00003 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00003_3_alpha=0.0707,encoder_types=mlp,ks=16,n_factors=15,rho=0.0234,s=1.2090,use_dynamic_alpha_rho=True_2023-03-31_13-50-42/error.txt  |\n",
      "| train_and_evaluate_868af_00004 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00004_4_alpha=0.0259,encoder_types=ngam,ks=8,n_factors=3,rho=0.0453,s=3.1354,use_dynamic_alpha_rho=True_2023-03-31_13-50-45/error.txt   |\n",
      "| train_and_evaluate_868af_00005 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00005_5_alpha=0.0095,encoder_types=ngam,ks=4,n_factors=2,rho=0.0971,s=0.5815,use_dynamic_alpha_rho=True_2023-03-31_13-50-46/error.txt   |\n",
      "| train_and_evaluate_868af_00006 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00006_6_alpha=0.0195,encoder_types=mlp,ks=16,n_factors=4,rho=0.0740,s=2.1358,use_dynamic_alpha_rho=False_2023-03-31_13-50-52/error.txt  |\n",
      "| train_and_evaluate_868af_00007 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00007_7_alpha=0.0202,encoder_types=mlp,ks=16,n_factors=6,rho=0.0328,s=2.0691,use_dynamic_alpha_rho=True_2023-03-31_13-50-53/error.txt   |\n",
      "| train_and_evaluate_868af_00008 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00008_8_alpha=0.0475,encoder_types=mlp,ks=16,n_factors=4,rho=0.0834,s=2.2968,use_dynamic_alpha_rho=False_2023-03-31_13-50-59/error.txt  |\n",
      "| train_and_evaluate_868af_00009 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00009_9_alpha=0.0042,encoder_types=mlp,ks=8,n_factors=2,rho=0.0050,s=4.9192,use_dynamic_alpha_rho=True_2023-03-31_13-50-59/error.txt    |\n",
      "| train_and_evaluate_868af_00010 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00010_10_alpha=0.0357,encoder_types=mlp,ks=4,n_factors=0,rho=0.0545,s=4.6294,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00011 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00011_11_alpha=0.0405,encoder_types=mlp,ks=8,n_factors=3,rho=0.0263,s=1.6794,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00012 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00012_12_alpha=0.0101,encoder_types=mlp,ks=16,n_factors=5,rho=0.0250,s=0.6624,use_dynamic_alpha_rho=False_2023-03-31_13-51-12/error.txt |\n",
      "| train_and_evaluate_868af_00013 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00013_13_alpha=0.0735,encoder_types=ngam,ks=4,n_factors=0,rho=0.0015,s=3.0955,use_dynamic_alpha_rho=True_2023-03-31_13-51-13/error.txt  |\n",
      "| train_and_evaluate_868af_00014 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00014_14_alpha=0.0711,encoder_types=mlp,ks=16,n_factors=3,rho=0.0108,s=1.0200,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt  |\n",
      "| train_and_evaluate_868af_00015 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00015_15_alpha=0.0503,encoder_types=mlp,ks=8,n_factors=6,rho=0.0448,s=4.2895,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt   |\n",
      "| train_and_evaluate_868af_00016 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00016_16_alpha=0.0891,encoder_types=mlp,ks=8,n_factors=1,rho=0.0039,s=2.4200,use_dynamic_alpha_rho=False_2023-03-31_13-51-26/error.txt  |\n",
      "| train_and_evaluate_868af_00017 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00017_17_alpha=0.0168,encoder_types=ngam,ks=16,n_factors=4,rho=0.0321,s=3.4477,use_dynamic_alpha_rho=True_2023-03-31_13-51-26/error.txt |\n",
      "| train_and_evaluate_868af_00018 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00018_18_alpha=0.0194,encoder_types=ngam,ks=4,n_factors=3,rho=0.0408,s=1.5218,use_dynamic_alpha_rho=False_2023-03-31_13-51-32/error.txt |\n",
      "| train_and_evaluate_868af_00019 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00019_19_alpha=0.0308,encoder_types=ngam,ks=8,n_factors=2,rho=0.0662,s=1.0104,use_dynamic_alpha_rho=True_2023-03-31_13-51-32/error.txt  |\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "== Status ==\n",
      "Current time: 2023-03-31 14:06:42 (running for 00:16:13.44)\n",
      "Memory usage on this node: 51.6/64.0 GiB \n",
      "Using AsyncHyperBand: num_stopped=0\n",
      "Bracket: Iter 64.000: None | Iter 16.000: None | Iter 4.000: None | Iter 1.000: None\n",
      "Resources requested: 1.0/2 CPUs, 0/1 GPUs, 0.0/38.21 GiB heap, 0.0/2.0 GiB objects\n",
      "Result logdir: /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29\n",
      "Number of trials: 310/500 (293 ERROR, 16 PENDING, 1 RUNNING)\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+-----------+---------+------------------------+\n",
      "| Trial name                     | status   | loc   |     alpha | encoder_types   |   ks |   n_factors |       rho |       s | use_dynamic_alpha_rh   |\n",
      "|                                |          |       |           |                 |      |             |           |         | o                      |\n",
      "|--------------------------------+----------+-------+-----------+-----------------+------+-------------+-----------+---------+------------------------|\n",
      "| train_and_evaluate_868af_00293 | RUNNING  |       | 0.0963094 | mlp             |    8 |           5 | 0.0790648 | 2.65924 | False                  |\n",
      "| train_and_evaluate_868af_00294 | PENDING  |       | 0.0671536 | ngam            |    4 |           0 | 0.0113879 | 2.12692 | True                   |\n",
      "| train_and_evaluate_868af_00295 | PENDING  |       | 0.0800513 | ngam            |    4 |           2 | 0.0672165 | 2.56692 | False                  |\n",
      "| train_and_evaluate_868af_00296 | PENDING  |       | 0.071429  | ngam            |    8 |           6 | 0.015706  | 2.79164 | False                  |\n",
      "| train_and_evaluate_868af_00297 | PENDING  |       | 0.0220178 | mlp             |    8 |           3 | 0.0216536 | 4.31771 | True                   |\n",
      "| train_and_evaluate_868af_00298 | PENDING  |       | 0.0349687 | ngam            |    4 |           0 | 0.0803356 | 3.01573 | True                   |\n",
      "| train_and_evaluate_868af_00000 | ERROR    |       | 0.0295006 | ngam            |    4 |           2 | 0.0956552 | 0.56271 | True                   |\n",
      "| train_and_evaluate_868af_00001 | ERROR    |       | 0.0966307 | ngam            |   16 |           9 | 0.0368085 | 1.91761 | False                  |\n",
      "| train_and_evaluate_868af_00002 | ERROR    |       | 0.0473017 | ngam            |    8 |           4 | 0.0168779 | 3.90863 | False                  |\n",
      "| train_and_evaluate_868af_00003 | ERROR    |       | 0.0706665 | mlp             |   16 |          15 | 0.0233823 | 1.20896 | True                   |\n",
      "| train_and_evaluate_868af_00004 | ERROR    |       | 0.0259421 | ngam            |    8 |           3 | 0.0452985 | 3.13542 | True                   |\n",
      "+--------------------------------+----------+-------+-----------+-----------------+------+-------------+-----------+---------+------------------------+\n",
      "... 300 more trials not shown (11 PENDING, 288 ERROR)\n",
      "Number of errored trials: 293\n",
      "Table truncated to 20 rows (273 overflow)\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name                     |   # failures | error file                                                                                                                                                                                                                      |\n",
      "|--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| train_and_evaluate_868af_00000 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00000_0_alpha=0.0295,encoder_types=ngam,ks=4,n_factors=2,rho=0.0957,s=0.5627,use_dynamic_alpha_rho=True_2023-03-31_13-50-31/error.txt   |\n",
      "| train_and_evaluate_868af_00001 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00001_1_alpha=0.0966,encoder_types=ngam,ks=16,n_factors=9,rho=0.0368,s=1.9176,use_dynamic_alpha_rho=False_2023-03-31_13-50-35/error.txt |\n",
      "| train_and_evaluate_868af_00002 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00002_2_alpha=0.0473,encoder_types=ngam,ks=8,n_factors=4,rho=0.0169,s=3.9086,use_dynamic_alpha_rho=False_2023-03-31_13-50-36/error.txt  |\n",
      "| train_and_evaluate_868af_00003 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00003_3_alpha=0.0707,encoder_types=mlp,ks=16,n_factors=15,rho=0.0234,s=1.2090,use_dynamic_alpha_rho=True_2023-03-31_13-50-42/error.txt  |\n",
      "| train_and_evaluate_868af_00004 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00004_4_alpha=0.0259,encoder_types=ngam,ks=8,n_factors=3,rho=0.0453,s=3.1354,use_dynamic_alpha_rho=True_2023-03-31_13-50-45/error.txt   |\n",
      "| train_and_evaluate_868af_00005 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00005_5_alpha=0.0095,encoder_types=ngam,ks=4,n_factors=2,rho=0.0971,s=0.5815,use_dynamic_alpha_rho=True_2023-03-31_13-50-46/error.txt   |\n",
      "| train_and_evaluate_868af_00006 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00006_6_alpha=0.0195,encoder_types=mlp,ks=16,n_factors=4,rho=0.0740,s=2.1358,use_dynamic_alpha_rho=False_2023-03-31_13-50-52/error.txt  |\n",
      "| train_and_evaluate_868af_00007 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00007_7_alpha=0.0202,encoder_types=mlp,ks=16,n_factors=6,rho=0.0328,s=2.0691,use_dynamic_alpha_rho=True_2023-03-31_13-50-53/error.txt   |\n",
      "| train_and_evaluate_868af_00008 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00008_8_alpha=0.0475,encoder_types=mlp,ks=16,n_factors=4,rho=0.0834,s=2.2968,use_dynamic_alpha_rho=False_2023-03-31_13-50-59/error.txt  |\n",
      "| train_and_evaluate_868af_00009 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00009_9_alpha=0.0042,encoder_types=mlp,ks=8,n_factors=2,rho=0.0050,s=4.9192,use_dynamic_alpha_rho=True_2023-03-31_13-50-59/error.txt    |\n",
      "| train_and_evaluate_868af_00010 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00010_10_alpha=0.0357,encoder_types=mlp,ks=4,n_factors=0,rho=0.0545,s=4.6294,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00011 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00011_11_alpha=0.0405,encoder_types=mlp,ks=8,n_factors=3,rho=0.0263,s=1.6794,use_dynamic_alpha_rho=True_2023-03-31_13-51-06/error.txt   |\n",
      "| train_and_evaluate_868af_00012 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00012_12_alpha=0.0101,encoder_types=mlp,ks=16,n_factors=5,rho=0.0250,s=0.6624,use_dynamic_alpha_rho=False_2023-03-31_13-51-12/error.txt |\n",
      "| train_and_evaluate_868af_00013 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00013_13_alpha=0.0735,encoder_types=ngam,ks=4,n_factors=0,rho=0.0015,s=3.0955,use_dynamic_alpha_rho=True_2023-03-31_13-51-13/error.txt  |\n",
      "| train_and_evaluate_868af_00014 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00014_14_alpha=0.0711,encoder_types=mlp,ks=16,n_factors=3,rho=0.0108,s=1.0200,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt  |\n",
      "| train_and_evaluate_868af_00015 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00015_15_alpha=0.0503,encoder_types=mlp,ks=8,n_factors=6,rho=0.0448,s=4.2895,use_dynamic_alpha_rho=True_2023-03-31_13-51-19/error.txt   |\n",
      "| train_and_evaluate_868af_00016 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00016_16_alpha=0.0891,encoder_types=mlp,ks=8,n_factors=1,rho=0.0039,s=2.4200,use_dynamic_alpha_rho=False_2023-03-31_13-51-26/error.txt  |\n",
      "| train_and_evaluate_868af_00017 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00017_17_alpha=0.0168,encoder_types=ngam,ks=16,n_factors=4,rho=0.0321,s=3.4477,use_dynamic_alpha_rho=True_2023-03-31_13-51-26/error.txt |\n",
      "| train_and_evaluate_868af_00018 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00018_18_alpha=0.0194,encoder_types=ngam,ks=4,n_factors=3,rho=0.0408,s=1.5218,use_dynamic_alpha_rho=False_2023-03-31_13-51-32/error.txt |\n",
      "| train_and_evaluate_868af_00019 |            1 | /Users/wtlo/ray_results/train_and_evaluate_2023-03-31_13-50-29/train_and_evaluate_868af_00019_19_alpha=0.0308,encoder_types=ngam,ks=8,n_factors=2,rho=0.0662,s=1.0104,use_dynamic_alpha_rho=True_2023-03-31_13-51-32/error.txt  |\n",
      "+--------------------------------+--------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate(config, data, strat_config, checkpoint_dir='checkpoints/'):\n",
    "    C,X,W = data\n",
    "    \n",
    "    config.update(strat_config)\n",
    "\n",
    "    n = W.shape[0]\n",
    "    p = W.shape[-1]\n",
    "    k = config['ks']\n",
    "    n_fit_iters = 1\n",
    "\n",
    "    header = \"n, p, k, data_gen, fit_iter, \"\n",
    "    header += \"recovery_pop_train, recovery_pop_test, mse_pop_train, mse_pop_test, \"\n",
    "    header += \"recovery_cluster_train, recovery_cluster_test, mse_cluster_train, mse_cluster_test, \"\n",
    "    header += \"recovery_notmad_notears_train, recovery_notmad_notears_test, mse_notmad_notears_train, mse_notmad_notears_test, \"\n",
    "    header += \"recovery_notmad_dagma_train, recovery_notmad_dagma_test, mse_notmad_dagma_train, mse_notmad_dagma_test, \"\n",
    "    header += \"recovery_notmad_poly_train, recovery_notmad_poly_test, mse_notmad_poly_train, mse_notmad_poly_test\"\n",
    "\n",
    "    C_train, C_test, X_train, X_test, W_train, W_test = train_test_split(C, X, W, test_size=0.3)\n",
    "\n",
    "    recovery_train = lambda W_pred: measure_recovery(W_train, W_pred)\n",
    "    recovery_test = lambda W_pred: measure_recovery(W_test, W_pred)\n",
    "    mse_train = lambda W_pred: np.mean(measure_mses(W_pred, X_train))\n",
    "    mse_test = lambda W_pred: np.mean(measure_mses(W_pred, X_test))\n",
    "    results_string = lambda W_pred_train, W_pred_test: f\", {recovery_train(W_pred_train)}, {recovery_test(W_pred_test)}, {mse_train(W_pred_train)}, {mse_test(W_pred_test)}\"\n",
    "    \n",
    "    test_mses = []\n",
    "    test_recoveries = []\n",
    "    train_mses = []\n",
    "    train_recoveries = []\n",
    "    # results_line = \"\"\n",
    "    for fit_iter in range(n_fit_iters):\n",
    "\n",
    "        if config['loss_type'] == 'NOTEARS':\n",
    "            loss_kwargs = {\n",
    "                'archetype_alpha': 0.0,\n",
    "                'archetype_rho': 0.0,\n",
    "                'sample_specific_alpha': config['alpha'],\n",
    "                'sample_specific_rho': config['rho'],\n",
    "            }\n",
    "        elif config['loss_type'] == 'DAGMA':\n",
    "            loss_kwargs = {\n",
    "                'archetype_alpha': 0.0,\n",
    "                'archetype_s': config['s'],\n",
    "                'sample_specific_alpha': config['alpha'],\n",
    "                'sample_specific_s': config['s']\n",
    "            }\n",
    "        elif config['loss_type'] == 'poly':\n",
    "            loss_kwargs = {}\n",
    "\n",
    "        cbn = ContextualizedBayesianNetworks(\n",
    "            encoder_type=config['encoder_types'],\n",
    "            num_archetypes=k,\n",
    "            num_factors=config['n_factors'],\n",
    "            archetype_dag_loss_type=config['loss_type'],\n",
    "            sample_specific_dag_loss_type=config['loss_type'],\n",
    "            **loss_kwargs,\n",
    "            n_bootstraps=3,\n",
    "            learning_rate=1e-3,\n",
    "        )\n",
    "        \n",
    "        cbn.fit(C_train, X_train, max_epochs=100)\n",
    "        cbn_preds_train = cbn.predict_networks(C_train, individual_preds=True)\n",
    "        cbn_preds_test = cbn.predict_networks(C_test, individual_preds=True)\n",
    "        \n",
    "        test_mses.append(mse_test(cbn_preds_test))\n",
    "        test_recoveries.append(recovery_test(cbn_preds_test))\n",
    "        train_mses.append(mse_train(cbn_preds_train))\n",
    "        train_recoveries.append(recovery_train(cbn_preds_train))\n",
    "    \n",
    "    train_mse = np.mean(train_mses)\n",
    "    train_recovery = np.mean(train_recoveries)\n",
    "    test_mse = np.mean(test_mses)\n",
    "    test_recovery = np.mean(test_recoveries)\n",
    "    \n",
    "    tune.report(train_mse=train_mse, train_recovery=train_recovery,test_mse=test_mse, test_recovery=test_recovery)\n",
    "\n",
    "\n",
    "#hyperparameter groups\n",
    "losses = [\"NOTEARS\", \"DAGMA\", \"poly\"]\n",
    "encoder_types = ['ngam', 'mlp']\n",
    "ks = [4, 8, 16]\n",
    "\n",
    "stratify_by = {\n",
    "    'loss_type': losses,\n",
    "    'encoder_types': encoder_types,\n",
    "    'ks':ks\n",
    "}\n",
    "\n",
    "# [3 x (500) num_models / samples] x [(2 + 3 + 2) hps] x [20 datasets] = 210,000 runs\n",
    "# / 3600 = ~60 hours Training time\n",
    "\n",
    "n_runs = 3\n",
    "\n",
    "for d in range(len(datas)):\n",
    "    for strat in stratify_by.keys():\n",
    "        config = {\n",
    "            \"loss_type\": tune.choice(losses),\n",
    "            \"encoder_types\": tune.choice(encoder_types),\n",
    "            'use_dynamic_alpha_rho': tune.choice([True, False]),\n",
    "            \"ks\": tune.choice(ks),\n",
    "            \"n_factors\": tune.sample_from(lambda spec: int(np.random.randint(0,spec.config.ks))),\n",
    "            \"alpha\": tune.uniform(0.001, 0.1),\n",
    "            \"rho\": tune.uniform(0.001, 0.1),\n",
    "            \"s\": tune.uniform(0.001, 5),\n",
    "        }\n",
    "\n",
    "        del config[strat]\n",
    "        if strat == 'ks':\n",
    "            del config['n_factors']\n",
    "\n",
    "        from ray.tune.schedulers import ASHAScheduler\n",
    "        from ray.tune import CLIReporter\n",
    "        from ray.air import RunConfig\n",
    "\n",
    "        for instance in stratify_by[strat]:\n",
    "            if strat == 'ks':\n",
    "                config['n_factors'] = tune.sample_from(lambda spec: int(np.random.randint(0,instance)))\n",
    "            \n",
    "            for i in range (n_runs):\n",
    "                \n",
    "                if not os.path.exists('data3/'): os.mkdir('data3/')\n",
    "                if os.path.exists(f'data3/tune_analysis_d{d}_s{strat}_ins{instance}_i{i}.pickle'): \n",
    "                    with open(f'data3/tune_analysis_d{d}_s{strat}_ins{instance}_i{i}.pickle', 'rb') as handle:\n",
    "                            r = pkl.load(handle)\n",
    "                            results = r.get_best_result()\n",
    "                            #redo broken runs\n",
    "                            if not len(results.metrics.keys()) == 1: continue\n",
    "                            print(f'redoing broken run: tune_analysis_d{d}_s{strat}_ins{instance}_i{i}')\n",
    "\n",
    "                reporter = CLIReporter(max_progress_rows=10)\n",
    "                reporter.add_metric_column(\"test_mse\")\n",
    "                reporter.add_metric_column(\"test_recovery\")\n",
    "                reporter.add_metric_column(\"train_mse\")\n",
    "                reporter.add_metric_column(\"train_recovery\")\n",
    "\n",
    "                tuner = tune.Tuner(\n",
    "                    functools.partial(train_and_evaluate, data=(datas[d]), strat_config={strat: instance}),\n",
    "                    param_space=config,\n",
    "                    run_config=RunConfig(\n",
    "                        progress_reporter=reporter\n",
    "                    ),\n",
    "                    tune_config=tune.TuneConfig(\n",
    "                        num_samples=500,\n",
    "                        scheduler=ASHAScheduler(),\n",
    "                        metric=\"test_mse\",\n",
    "                        mode='min',\n",
    "                    ),\n",
    "                )\n",
    "                results = tuner.fit()\n",
    "                \n",
    "                with open(f'data3/tune_analysis_d{d}_s{strat}_ins{instance}_i{i}.pickle', 'wb') as handle:\n",
    "                    pkl.dump(results, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81340340",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.csv\", 'w') as out_file:\n",
    "    print(header, file=out_file)\n",
    "    for n in ns:\n",
    "        for p in ps:\n",
    "            for k in ks:\n",
    "                for data_gen in range(n_data_gens):\n",
    "                    # Generate data.\n",
    "                    C = np.random.normal(0, 1, size=(n, 1))\n",
    "                    W = np.zeros((p, p, n, 1))\n",
    "\n",
    "                    # TODO: Automate generation of W -- clusters?\n",
    "                    W[0, 1] = C - 2\n",
    "                    W[2, 1] = C**2\n",
    "                    W[3, 1] = C**3\n",
    "                    W[3, 2] = C\n",
    "                    \n",
    "                    W = np.squeeze(W)\n",
    "                    W = np.transpose(W, (2, 0, 1))\n",
    "                    X = np.array([simulate_linear_sem(w, n_samples=1, sem_type=\"uniform\", noise_scale=0.0)[0] for w in W])\n",
    "\n",
    "                    # print(X.shape)\n",
    "                    # print(X)\n",
    "\n",
    "\n",
    "                    # fdsafd\n",
    "                    C_train, C_test, X_train, X_test, W_train, W_test = train_test_split(C, X, W, test_size=0.3)\n",
    "                    \n",
    "                    recovery_train = lambda W_pred: measure_recovery(W_train, W_pred)\n",
    "                    recovery_test = lambda W_pred: measure_recovery(W_test, W_pred)\n",
    "                    mse_train = lambda W_pred: np.mean(measure_mses(W_pred, X_train))\n",
    "                    mse_test = lambda W_pred: np.mean(measure_mses(W_pred, X_test))\n",
    "                    results_string = lambda W_pred_train, W_pred_test: f\", {recovery_train(W_pred_train)}, {recovery_test(W_pred_test)}, {mse_train(W_pred_train)}, {mse_test(W_pred_test)}\"\n",
    "\n",
    "                    for fit_iter in range(n_fit_iters):\n",
    "                        results_line = f\"{n}, {p}, {k}, {data_gen}, {fit_iter}\"\n",
    "                        \n",
    "                        dag = BayesianNetwork().fit(X_train, max_epochs=100)\n",
    "                        pop_preds_train = np.expand_dims(dag.predict(len(X_train)), 0)\n",
    "                        pop_preds_test  = np.expand_dims(dag.predict(len(X_test)), 0)\n",
    "                        results_line += results_string(pop_preds_train, pop_preds_test)\n",
    "\n",
    "                        km = KMeans(n_clusters=4)\n",
    "                        km.fit(C)\n",
    "                        cluster_dag = GroupedNetworks(BayesianNetwork).fit(X_train, km.predict(C_train))\n",
    "                        cluster_preds_train = np.expand_dims(cluster_dag.predict(km.predict(C_train)), 0)\n",
    "                        cluster_preds_test = np.expand_dims(cluster_dag.predict(km.predict(C_test)), 0)\n",
    "                        results_line += results_string(cluster_preds_train, cluster_preds_test)\n",
    "\n",
    "                        for loss in losses:\n",
    "                            cbn = ContextualizedBayesianNetworks(\n",
    "                                encoder_type='ngam',\n",
    "                                num_archetypes=k,\n",
    "                                num_factors=-1,\n",
    "                                archetype_alpha=0.,\n",
    "                                archetype_rho=0.,\n",
    "                                sample_specific_alpha=1e-1,\n",
    "                                sample_specific_rho=1e-2,\n",
    "                                archetype_dag_loss_type=loss,\n",
    "                                sample_specific_dag_loss_type=loss,\n",
    "                                n_bootstraps=3,\n",
    "                                learning_rate=1e-3)\n",
    "                            cbn.fit(C_train, X_train, max_epochs=100)\n",
    "                            cbn_preds_train = cbn.predict_networks(C_train, individual_preds=True)\n",
    "                            cbn_preds_test = cbn.predict_networks(C_test, individual_preds=True)\n",
    "                            results_line += results_string(cbn_preds_train, cbn_preds_test)\n",
    "                        \n",
    "                        print(results_line, file=out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e0c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df.columns)\n",
    "ns = list(set(results_df['n'].values))\n",
    "ps = list(set(results_df[' p'].values))\n",
    "ks = list(set(results_df[' k'].values))\n",
    "data_gens = list(set(results_df[' data_gen'].values))\n",
    "fit_iters = list(set(results_df[' fit_iter'].values))\n",
    "\n",
    "# Plot error by n\n",
    "for p in ps:\n",
    "    fig = plt.figure()\n",
    "    pop_errs_to_plot = np.zeros((len(ns), len(data_gens)*len(fit_iters)*len(ks)))\n",
    "    cluster_errs_to_plot = np.zeros((len(ns), len(data_gens)*len(fit_iters)*len(ks)))\n",
    "    notmad_notears_errs_to_plot = {k: np.zeros((len(ns), len(data_gens)*len(fit_iters))) for k in ks}\n",
    "    for i, n in enumerate(ns):\n",
    "        for data_gen in data_gens:\n",
    "            plot_idxs_start = data_gen*len(fit_iters)*len(ks)\n",
    "            plot_idxs_end = (data_gen+1)*len(fit_iters)*len(ks)\n",
    "            \n",
    "            idxs = np.logical_and(\n",
    "                results_df['n'] == n,\n",
    "                np.logical_and(\n",
    "                    results_df[' p'] == p,\n",
    "                    results_df[' data_gen'] == data_gen)\n",
    "            )\n",
    "            pop_errs = results_df[' recovery_pop_test'].loc[idxs] # 3 fit_iters, 3 ks\n",
    "            pop_err = np.mean(pop_errs)\n",
    "            pop_errs_to_plot[i, plot_idxs_start:plot_idxs_end] = 1\n",
    "            \n",
    "            cluster_errs = results_df[' recovery_cluster_test'].loc[idxs]\n",
    "            cluster_errs /= pop_err\n",
    "            cluster_errs_to_plot[i, plot_idxs_start:plot_idxs_end] = cluster_errs\n",
    "            \n",
    "            for k in ks:\n",
    "                notmad_notears_errs = results_df[' recovery_notmad_notears_test'].loc[np.logical_and(\n",
    "                    idxs,\n",
    "                    results_df[' k'] == k\n",
    "                )]\n",
    "                notmad_notears_errs /= pop_err\n",
    "                notmad_notears_errs_to_plot[k][i, data_gen*len(fit_iters):(data_gen+1)*len(fit_iters)] = notmad_notears_errs\n",
    "\n",
    "    #print(pop_errs_to_plot.shape)\n",
    "    plt.errorbar(ns,\n",
    "                 np.mean(pop_errs_to_plot, axis=1),\n",
    "                 yerr=2*np.std(pop_errs_to_plot, axis=1), label=\"Population\")\n",
    "    plt.errorbar(ns,\n",
    "                 np.mean(cluster_errs_to_plot, axis=1),\n",
    "                 yerr=2*np.std(cluster_errs_to_plot, axis=1), label=\"Cluster\")\n",
    "    for k in ks:\n",
    "        plt.errorbar(ns,\n",
    "                 np.mean(notmad_notears_errs_to_plot[k], axis=1),\n",
    "                 yerr=2*np.std(notmad_notears_errs_to_plot[k], axis=1), label=f\"NOTMAD-NOTEARS-{k}\")\n",
    "    plt.legend()\n",
    "    plt.title(p)\n",
    "    plt.xlabel(\"N\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89ac59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: How to compare across data gen runs? Probably normalize by population error.\n",
    "def plot_mses(mses, title):\n",
    "    plt.figure()\n",
    "    plt.bar(\n",
    "        [0, 1, 2],\n",
    "        [np.mean(mses[\"pop\"]), np.mean(mses[\"cluster\"]), np.mean(mses[\"notmad\"])],\n",
    "        yerr=[2*np.std(mses[\"pop\"]), 2*np.std(mses[\"cluster\"]), 2*np.std(mses[\"notmad\"])]\n",
    "    )\n",
    "    plt.xticks([0, 1, 2], [\"Population\", \"Cluster\", \"NOTMAD\"], rotation=45, fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "for n in ns:\n",
    "    for p in ps:\n",
    "        for data_gen in range(n_data_gens):\n",
    "            mses_train = results[(n, p, data_gen, \"train\")]\n",
    "            mses_test = results[(n, p, data_gen, \"test\")]\n",
    "\n",
    "            plot_mses(mses_train, f\"n={n}, p={p}, Train\")\n",
    "            plot_mses(mses_test, f\"n={n}, p={p}, Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
