{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch VS TF Notmad ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "from contextualized.dags.torch_notmad.callbacks import *\n",
    "from contextualized.dags.torch_notmad.torch_notmad import NOTMAD_model\n",
    "from contextualized.dags.torch_notmad.datamodules import CXW_DataModule\n",
    "from contextualized.dags.torch_notmad.test_notmad.test_data import create_test_cxw_network\n",
    "from contextualized.modules import Explainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NGAM.__init__() got an unexpected keyword argument 'n_hidden_layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\satur\\Documents\\MEGA\\Summer22\\github\\Contextualized_Torch\\Contextualized-Torch\\contextualized\\dags\\torch_notmad\\torch_nm_demo.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=13'>14</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39m6\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=14'>15</a>\u001b[0m INIT_MAT \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39muniform(\u001b[39m-\u001b[39m\u001b[39m0.01\u001b[39m, \u001b[39m0.01\u001b[39m, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=15'>16</a>\u001b[0m                              size\u001b[39m=\u001b[39m(k,\u001b[39m4\u001b[39m,\u001b[39m4\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m NOTMAD_model(datamodule,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=18'>19</a>\u001b[0m                      init_mat\u001b[39m=\u001b[39;49mINIT_MAT,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=19'>20</a>\u001b[0m                      n_archetypes\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=20'>21</a>\u001b[0m                )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=21'>22</a>\u001b[0m ModelSummary(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/satur/Documents/MEGA/Summer22/github/Contextualized_Torch/Contextualized-Torch/contextualized/dags/torch_notmad/torch_nm_demo.ipynb#ch0000003?line=23'>24</a>\u001b[0m trainer\u001b[39m.\u001b[39mtune(model)\n",
      "File \u001b[1;32mc:\\Users\\satur\\Documents\\MEGA\\Summer22\\github\\Contextualized_Torch\\Contextualized-Torch\\contextualized\\dags\\torch_notmad\\torch_notmad.py:121\u001b[0m, in \u001b[0;36mNOTMAD_model.__init__\u001b[1;34m(self, datamodule, n_archetypes, sample_specific_loss_params, archetype_loss_params, learning_rate, opt_step, encoder_kwargs, init_mat, auto_opt, encoder_type)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_mat \u001b[39m=\u001b[39m init_mat\n\u001b[0;32m    120\u001b[0m \u001b[39m#layers\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39m=\u001b[39m NGAM(encoder_input_shape[\u001b[39m0\u001b[39;49m], encoder_output_shape[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m    122\u001b[0m                         n_hidden_layers\u001b[39m=\u001b[39;49mencoder_kwargs[\u001b[39m'\u001b[39;49m\u001b[39mlayers\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[0;32m    123\u001b[0m                         width\u001b[39m=\u001b[39;49mencoder_kwargs[\u001b[39m'\u001b[39;49m\u001b[39mwidth\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplainer \u001b[39m=\u001b[39m Explainer(arch_shape, init_mat\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_mat) \n\u001b[0;32m    126\u001b[0m \u001b[39m#loss\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: NGAM.__init__() got an unexpected keyword argument 'n_hidden_layers'"
     ]
    }
   ],
   "source": [
    "#Train notmad\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "\n",
    "C, X, W = create_test_cxw_network(n=500)\n",
    "datamodule = CXW_DataModule(C, X, W)\n",
    "\n",
    "trainer = Trainer(max_epochs = 5)\n",
    "\n",
    "k = 6\n",
    "INIT_MAT = np.random.uniform(-0.01, 0.01, \n",
    "                             size=(k,4,4))\n",
    "\n",
    "model = NOTMAD_model(datamodule,\n",
    "                     init_mat=INIT_MAT,\n",
    "                     n_archetypes=k,\n",
    "               )\n",
    "ModelSummary(model)\n",
    "\n",
    "trainer.tune(model)\n",
    "trainer.fit(model, datamodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train L2: 0.1314468311272936\n",
      "test L2:  0.13175667931519097\n",
      "train mse: 0.02104613928705072\n",
      "test mse:  0.020614418241872864\n"
     ]
    }
   ],
   "source": [
    "#Evaluate results\n",
    "C_train = trainer.model.datamodule.C_train\n",
    "C_test = trainer.model.datamodule.C_test\n",
    "W_train = trainer.model.datamodule.W_train\n",
    "W_test = trainer.model.datamodule.W_test\n",
    "X_train = trainer.model.datamodule.X_train\n",
    "X_test = trainer.model.datamodule.X_test\n",
    "\n",
    "torch_notmad_preds_train = trainer.model.predict_w(C_train, confirm_project_to_dag=True)\n",
    "torch_notmad_preds = trainer.model.predict_w(C_test).squeeze().detach().numpy()\n",
    "\n",
    "mse = lambda true, pred: ((true - pred)**2).mean()\n",
    "dag_pred = lambda x, w: np.matmul(x, w).squeeze()\n",
    "dags_pred = lambda xs, w: [dag_pred(x,w) for x in xs]\n",
    "\n",
    "example_preds = dags_pred(X_train, torch_notmad_preds_train)\n",
    "actual_preds = dags_pred(X_train, W_train)\n",
    "# print(example_preds[0])\n",
    "# print(actual_preds[0])\n",
    "# print(mse(example_preds[0],actual_preds[0]))\n",
    "print(f\"train L2: {mse(torch_notmad_preds_train, W_train)}\")\n",
    "print(f\"test L2:  {mse(torch_notmad_preds, W_test)}\")\n",
    "print(f\"train mse: {mse(dag_pred(X_train, torch_notmad_preds_train), X_train)}\")\n",
    "print(f\"test mse:  {mse(dag_pred(X_test, torch_notmad_preds), X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_notmad.tf_notmad import NOTMAD as tf_notmad_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00ab01bd2244b50b3e9c0a2049005e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NOTMAD Training:   0%|           0/100 ETA: ?s,  ?epochs/s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "11/11 [==============================] - 0s 757us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 551us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 724us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 652us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 653us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 500us/step\n",
      "11/11 [==============================] - 0s 709us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 608us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "11/11 [==============================] - 0s 900us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 800us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 700us/step\n",
      "11/11 [==============================] - 0s 600us/step\n",
      "11/11 [==============================] - 0s 700us/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#fit notmad\n",
    "\n",
    "def fit_notmad(sample_specific_loss_params, archetype_loss_params, \n",
    "                  C_train, X_train, k, project, notears_pop):\n",
    "    \n",
    "    init_mat = np.random.uniform(-0.01, 0.01, size=(k, X_train.shape[-1], X_train.shape[-1]))\n",
    "    make_notmad = lambda: tf_notmad_model(\n",
    "        C_train.shape, X_train.shape, k,\n",
    "        sample_specific_loss_params, archetype_loss_params,\n",
    "        n_encoder_layers=2, encoder_width=32,\n",
    "        activation='linear', #init_mat=init_mat,\n",
    "        project_archs_to_dag=False,\n",
    "        learning_rate=1e-3, #project_archs_to_dag=project,\n",
    "        project_distance=1.0,\n",
    "        context_activity_regularizer=tf.keras.regularizers.l1(0),\n",
    "        use_compatibility=False, update_compat_by_grad=False,\n",
    "        pop_model=None\n",
    "    )\n",
    "    notmad_ = make_notmad()\n",
    "    notmad_.fit(\n",
    "        C_train, X_train, batch_size=1, epochs=100, \n",
    "        es_patience=2, verbose=1\n",
    "    )\n",
    "    return notmad_\n",
    "\n",
    "k = 5\n",
    "loss_params = {'l1': 0., 'init_alpha': 1e-1, 'init_rho': 1e-2}\n",
    "# loss_params = {'l1': 1e-2, 'alpha': 1e-1, 'rho': 1e-2}\n",
    "archetype_loss_params = {'l1': 0., 'alpha': 1e-1, 'rho': 1e-2}\n",
    "\n",
    "tf_notmad = fit_notmad(\n",
    "    loss_params, archetype_loss_params,\n",
    "    C_train, X_train, k, project=True, \n",
    "    notears_pop=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "train L2: 0.12782335274382325\n",
      "test L2:  0.13142865585619545\n",
      "train mse: 0.021978381122471947\n",
      "test mse:  0.021363331720332283\n"
     ]
    }
   ],
   "source": [
    "#eval tf\n",
    "dag_pred = lambda x, w: np.matmul(x, w).squeeze()\n",
    "mse = lambda true, pred: ((true - pred)**2).mean()\n",
    "\n",
    "tf_notmad_preds_train = tf_notmad.predict_w(C_train, project_to_dag=False).squeeze()\n",
    "tf_notmad_preds = tf_notmad.predict_w(C_test, project_to_dag=False).squeeze()\n",
    "\n",
    "# print(tf_notmad_preds_train[0])\n",
    "# print(W_train[0])\n",
    "print(f\"train L2: {mse(tf_notmad_preds_train, W_train)}\")\n",
    "print(f\"test L2:  {mse(tf_notmad_preds, W_test)}\")\n",
    "print(f\"train mse: {mse(dag_pred(X_train, tf_notmad_preds_train), X_train)}\")\n",
    "print(f\"test mse:  {mse(dag_pred(X_test, tf_notmad_preds), X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true: [[ 0.         -0.05811623  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          3.77091257  0.          0.        ]\n",
      " [ 0.          7.3226739   1.94188377  0.        ]]\n",
      "torch: [[ 0.         -0.5559784   0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.04470294  4.164861    0.          0.        ]\n",
      " [-0.05763132  4.68571     1.8459836   0.        ]]\n",
      "tf: [[ 0.0000000e+00 -2.8542557e-01  2.5538774e-02 -5.1620044e-03]\n",
      " [-1.2898748e-02  0.0000000e+00  1.5390925e-03  1.6665417e-03]\n",
      " [ 6.2200982e-02  3.8569226e+00  0.0000000e+00  4.6608485e-03]\n",
      " [-5.0657275e-03  4.3348575e+00  1.7843938e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.30260521  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          2.88114907  0.          0.        ]\n",
      " [ 0.          4.89044742  1.69739479  0.        ]]\n",
      "torch: [[ 0.         -0.4239642   0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.03427     3.1756208   0.          0.        ]\n",
      " [-0.04419332  3.5727415   1.4075768   0.        ]]\n",
      "tf: [[ 0.0000000e+00 -3.5359722e-01  2.2629511e-02 -2.7772798e-03]\n",
      " [-1.5941434e-02  0.0000000e+00  1.8235393e-03  4.0385542e-03]\n",
      " [ 7.0479780e-02  3.4360266e+00  0.0000000e+00  8.0639329e-03]\n",
      " [-1.6833834e-03  3.8371186e+00  1.6151716e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.04609218  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          3.81775575  0.          0.        ]\n",
      " [ 0.          7.4595428   1.95390782  0.        ]]\n",
      "torch: [[ 0.         -0.56258476  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.0452247   4.2143664   0.          0.        ]\n",
      " [-0.05830374  4.741407    1.8679231   0.        ]]\n",
      "tf: [[ 0.0000000e+00 -2.8386137e-01  2.5606537e-02 -5.2174195e-03]\n",
      " [-1.2824277e-02  0.0000000e+00  1.5324421e-03  1.6091813e-03]\n",
      " [ 6.2007844e-02  3.8667178e+00  0.0000000e+00  4.5784600e-03]\n",
      " [-5.1451242e-03  4.3464489e+00  1.7883435e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.04809619  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          3.80992847  0.          0.        ]\n",
      " [ 0.          7.4366139   1.95190381  0.        ]]\n",
      "torch: [[ 0.         -0.56148314  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.04513771  4.206112    0.          0.        ]\n",
      " [-0.05819162  4.73212     1.864265    0.        ]]\n",
      "tf: [[ 0.0000000e+00 -2.8411454e-01  2.5595561e-02 -5.2084452e-03]\n",
      " [-1.2836357e-02  0.0000000e+00  1.5335191e-03  1.6184822e-03]\n",
      " [ 6.2039126e-02  3.8651309e+00  0.0000000e+00  4.5918198e-03]\n",
      " [-5.1322617e-03  4.3445711e+00  1.7877036e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.14829659  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          3.42880551  0.          0.        ]\n",
      " [ 0.          6.34913084  1.85170341  0.        ]]\n",
      "torch: [[ 0.         -0.50668263  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.04080865  3.7954602   0.          0.        ]\n",
      " [-0.05261365  4.270107    1.6822753   0.        ]]\n",
      "tf: [[ 0.0000000e+00 -3.0127829e-01  2.4855675e-02 -4.6029040e-03]\n",
      " [-1.3636698e-02  0.0000000e+00  1.6060477e-03  2.2372934e-03]\n",
      " [ 6.4146958e-02  3.7581511e+00  0.0000000e+00  5.4803151e-03]\n",
      " [-4.2675156e-03  4.2179990e+00  1.7446064e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.93186373  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          1.1409151   0.          0.        ]\n",
      " [ 0.          1.2186528   1.06813627  0.        ]]\n",
      "torch: [[ 0.         -0.1433239   0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.01204477  1.0728438   0.          0.        ]\n",
      " [-0.01561941  1.2069578   0.47564664  0.        ]]\n",
      "tf: [[ 0.         -0.7832107   0.0053163   0.0115411 ]\n",
      " [-0.03039555  0.          0.00348952  0.01601155]\n",
      " [ 0.11942854  0.92285764  0.          0.02514521]\n",
      " [ 0.017819    0.8734243   0.61657465  0.        ]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.80961924  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          1.41700636  0.          0.        ]\n",
      " [ 0.          1.68677711  1.19038076  0.        ]]\n",
      "torch: [[ 0.         -0.18724878  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.01552597  1.4019518   0.          0.        ]\n",
      " [-0.02009207  1.5772293   0.6215059   0.        ]]\n",
      "tf: [[ 0.         -0.7299709   0.00730021  0.00987953]\n",
      " [-0.02935204  0.          0.00330322  0.01499903]\n",
      " [ 0.11387218  1.2122227   0.          0.02371973]\n",
      " [ 0.01568928  1.213283    0.729572    0.        ]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.43086172  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          2.46219493  0.          0.        ]\n",
      " [ 0.          3.86352431  1.56913828  0.        ]]\n",
      "torch: [[ 0.         -0.35778558  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.02903466  2.6797352   0.          0.        ]\n",
      " [-0.03745601  3.0148327   1.1878091   0.        ]]\n",
      "tf: [[ 0.0000000e+00 -4.3182054e-01  1.9348728e-02 -8.0763668e-05]\n",
      " [-1.9167181e-02  0.0000000e+00  2.1428389e-03  6.5929270e-03]\n",
      " [ 7.9797603e-02  2.9609015e+00  0.0000000e+00  1.1723237e-02]\n",
      " [ 2.0956735e-03  3.2757201e+00  1.4248109e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.41282565  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          2.51912241  0.          0.        ]\n",
      " [ 0.          3.99828648  1.58717435  0.        ]]\n",
      "torch: [[ 0.         -0.36690634  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.02975643  2.7480772   0.          0.        ]\n",
      " [-0.03838458  3.091723    1.2180972   0.        ]]\n",
      "tf: [[ 0.0000000e+00 -4.1877180e-01  1.9892856e-02 -5.2839256e-04]\n",
      " [-1.8643651e-02  0.0000000e+00  2.0899649e-03  6.1760060e-03]\n",
      " [ 7.8253217e-02  3.0397291e+00  0.0000000e+00  1.1126286e-02]\n",
      " [ 1.4708695e-03  3.3688352e+00  1.4563563e+00  0.0000000e+00]]\n",
      "---------------------------\n",
      "true: [[ 0.         -0.96793587  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          1.06515636  0.          0.        ]\n",
      " [ 0.          1.09930968  1.03206413  0.        ]]\n",
      "torch: [[ 0.         -0.1315707   0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.01111379  0.984781    0.          0.        ]\n",
      " [-0.01442267  1.1078806   0.43661782  0.        ]]\n",
      "tf: [[ 0.         -0.79432106  0.00492448  0.01187231]\n",
      " [-0.03051062  0.          0.00352561  0.01615812]\n",
      " [ 0.12051816  0.86550564  0.          0.02534774]\n",
      " [ 0.01822402  0.8062676   0.59447175  0.        ]]\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "#Compare predictions\n",
    "for i in range(10):\n",
    "    print(f'True: {W_train[i]}')\n",
    "    print(f'Torch: {torch_notmad_preds_train[i]}')\n",
    "    print(f'TF: {tf_notmad_preds_train[i]}')\n",
    "    print('---------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "416d0f5517e058c2412d4243473937e230461a2e3cd62050ec8d8e3887132890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
